{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0731bd83",
   "metadata": {},
   "source": [
    "# Final Submission: Loan Dataset Analysis and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebdcb0b",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b846b",
   "metadata": {},
   "source": [
    "### Description:\n",
    "We first load the dataset and check its structure to identify missing values, categorical columns, and initial patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'path_to_training_dataset.csv'  # Replace with your file path\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(train_data.info())\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5887f5",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe693e3a",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Remove unnecessary columns, handle missing values, and clean categorical data.\n",
    "\n",
    "#### Actions:\n",
    "- Drop irrelevant columns.\n",
    "- Clean categorical columns by removing 'Unknown'.\n",
    "- Impute missing numeric values with their median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['id', 'member_id', 'desc', 'mths_since_last_major_derog', 'application_approved_flag']\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Clean and convert 'emp_length'\n",
    "    def clean_emp_length(value):\n",
    "        if pd.isnull(value) or value == \"n/a\":\n",
    "            return np.nan\n",
    "        elif \"<\" in value:\n",
    "            return 0\n",
    "        elif \"10+\" in value:\n",
    "            return 10\n",
    "        else:\n",
    "            try:\n",
    "                return int(value.split()[0])\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "    if 'emp_length' in df.columns:\n",
    "        df['emp_length'] = df['emp_length'].apply(clean_emp_length)\n",
    "\n",
    "    # Impute numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "\n",
    "    # Drop rows with 'Unknown' in categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df = df[df[col] != 'Unknown']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data_cleaned = preprocess_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f9770",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efadc8",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Analyze data distributions and relationships. This helps identify trends, correlations, and potential predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f775456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for numeric features\n",
    "numeric_columns = train_data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.histplot(train_data_cleaned[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "correlation_matrix = train_data_cleaned.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f95e7",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b27d1c",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Encode categorical features, handle multicollinearity, and address class imbalance.\n",
    "\n",
    "#### Actions:\n",
    "- Use `OneHotEncoder` for categorical data.\n",
    "- Drop redundant features.\n",
    "- Apply SMOTE to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ebfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train_data_cleaned.drop(columns=['bad_flag'])\n",
    "y = train_data_cleaned['bad_flag']\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_columns = X.select_dtypes(include=['category', 'object']).columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_categorical = pd.DataFrame(\n",
    "    encoder.fit_transform(X[categorical_columns]),\n",
    "    columns=encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "\n",
    "# Drop original categorical columns and merge with encoded data\n",
    "X_numeric = X.drop(columns=categorical_columns).reset_index(drop=True)\n",
    "X_encoded = pd.concat([X_numeric, encoded_categorical], axis=1)\n",
    "\n",
    "# Handle multicollinearity by dropping highly correlated features\n",
    "X_encoded = X_encoded.drop(columns=['tot_hi_cred_lim'], errors='ignore')\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_encoded, y)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class Distribution After SMOTE:\")\n",
    "print(y_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c13358",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565910c",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Split the balanced dataset into training and validation sets for model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check dataset shapes\n",
    "print(f\"Training Data Shape: {X_train.shape}, Validation Data Shape: {X_val.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
